---
title: "Statistical Learning for Data Science Exam"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, error=TRUE)
```

The duration of the exam is 120 minutes. For most exercises you are required to provide R code: write your code in the assigned R code chunk. For some exercises you are required to provide a textual answer: type your answer in the space that follows 

**Your answer**.

In the following R chunk please fill in the required information otherwise the file will not be knitted

```{r, error=FALSE}
matricola <- 123456
#write your name within quotes, i.e. name <- "your name"
name <- "none"
#write your suname within quotes, i.e. name <- "your surname"
surname <- "none"
if (!exists("matricola") | !exists("name") | !exists("surname"))
  stop("Please, insert your information in the first chunk.")
```

```{r, cache=FALSE}
library(SLDS2425)
library(tidyverse)
```

# Exercise 1

## a)

**Your answer**:
```{r}

```


## b) 

**Your answer**:
```{r}

```

## c)

**Your answer**:
```{r}


```

## d) 

**Your answer**:
```{r}


```


## e) 

**Your answer**:

```{r}

  
```


# Exercise 2
A data analyst has processed some data regarding supermarket sales (in dollars) of two categories of pasta, "FreshCutPasta" and "FreshNoodles". The response variable is a dummy variable that takes the value '0' if the sales of "FreshCutPasta" are greater than those of "FreshNoodles" and '1' otherwise. The covariate is categorical and refers to the different ingredients used to make the pasta: 'plain' (wheat) and "whole" (whole wheat). In the chunk below you are given some information regarding the output of the analysis, in particular

  * the summary of a logistic regression model estimated by the analyst. Some entries are purposedly shown as 'NA'
  * the vector with the values of the response variable (the variable containing the values is called `pasta`)
  * the vector with the values of the explanatory variable (the variable containing the values is called `flavour`)

To visualize the summary and get access to the variables containing the values of the residuals and of the explanatory variable *you have to compile the code*.
  
```{r echo=TRUE}
library(SLDS2425)
set.seed(matricola)
obj <- slds.fun(matricola, 1)
pasta <- obj$pasta
flavour <- obj$flavour
```

## (a) 
By using the information included in the summary above, provide the interpretation of the coefficients in the model in terms of the odds ratio (no coding needed, just plain text)

**Your answer**:
The odds ratio is defined as
$$ \frac{P(Y=1|\text{flavour=z})}{P(Y=0|\text{flavour=z})}=\exp({\beta_0+\beta_1 z})$$
where $\beta_0$ and $\beta_1$ are the parameters of the model and $z$ is the specific values assumed with the covariate `flavour`. Since the covariate is categorical with two categories, it can be viewed as a dummy variable with two levels. By looking at the summary, we have that the reference category is `wheat`, meaning that when `flavour=wheat` then z=0, whereas when `flavour=wheat` then z=1. This entails that the intercept is gauging what is the change in the odds ratio when the ingredient is just `wheat`. The coefficient associated to the category `whole wheat` gauges how the odds ratio is changing with respect to the reference category. If the coefficient for category `whole wheat` is greater than zero then the odds for that category are higher than for the reference category, lower otherwise. In the given example, when the ingredient is `whole wheat`, the chances to sell more "FreshNoodles" than "FreshCutPasta" (Y=1) are lower compared to the case where thge ingredient is `wheat`.


## (b)
By using the information included in the summary of the estimated logistic regression model, state whether it is more likely to sell more `FreshNoodles` than `FreshCutPasta` when the ingredient is `wheat` or `whole wheat`

**Your answer**:
If we sell more `FreshNoodles` than `FreshCutPasta` then the response variable is equal to 1 by its definition. We are asked to compute the probability, according to the model, of the response being equal to 2 when `flavour` is equal to `wheat` or `whole wheat`. Those probabilities are equal to the logistic function for each category of `flavour`

  * $P(Y=1|\text{flavour=wheat})$= exp(1.3863)/(1+exp(1.3863))=0.8
  * $P(Y=1|\text{flavour=whole wheat})$= exp(1.3863-2.2336)/(1+exp(1.3863-2.2336))=0.3

It is more likely to sell more `FreshNoodles` than `FreshCutPasta` when the ingredient is just wheat.

## (c)
Given that the significance of the coefficients is not given in the summary, run a suitable exploratory analysis in order to gain some insights regarding the usefulness of the explanatory variable. Motivate your answer.

**Your answer**:
```{r echo=TRUE}
prop.table(table(flavour, pasta), margin=2)
```

We build a conditional contingecy table, specifically we display the conditional distribution of the explanatory variable by conditioning of the response's categories. From the output it is clear that the conditional distribution of the variable `flavour` changes according to the categories of the response. For instance, when the response is equal to 0 ("FreshCutPasta">"FreshNoodles") it is more likely that the ingredient is wheat (~73%), whereas when the response is equal to 1 ("FreshCutPasta"<"FreshNoodles") it is more likely that the ingredient is whole wheat (~73%). So we might expect to estimate a meaningful logistic regression model by considering the explanatory variable `flavour`.

## (d)
Establish whether a logistic regression model with the intercept only is to be preferred to the model that includes the explanatory variable `flavour`. Estimate the models, compare them by using a suitable metric and motivate your answer.


**Your answer**:
```{r echo=TRUE}
fit.null <- glm(pasta~1, family = binomial)
fit <- glm(pasta~flavour, family = binomial)
AIC(fit.null)
AIC(fit)
```

We estimate the models to be compared by using the data provided in the first chunk of exercise 2. The model with the intercept only is the model that does not include the explanatory variable. We perform the comparison between the two models by using the Akaike information criterion and choose the model with smalles AIC. So the model with the intercept only has not to be preferred to the one including the explanatory variable `flavour`.


# Exercise 3
The `leukemia` dataset contains gene expression data to be used to classify cancer types. The available data consists of 72 tissue samples biopsy (the statistical units) and of 3572 covariates, corresponding to the expression of 3572 genes. The dataset contains

  * The response variable `Y`: it takes on the values 0 (no cancer) and 1 (cancer)
  * The predictors `x.1` to `x.3572`: each predictor take values on the real line. The values indicates the level of expression of the corresponding gene.

The aim of the analysis consists in building a classifier to predict the whether a patient has a tumor based on a tissue sample biopsy and the gene expression data that can be obtained from it.


## (a) 
Do not modify this chunk.
```{r echo=TRUE}
library(SLDS2425)
data(leukemia)
out <- slds.fun(matricola, 2, leukemia)
```

A Lasso regression model has been fit to the data. The best fit includes just two covariates, not including the intercept, and the estimated coefficients are

  * paste0(`r out$v1`,"=", `r out$e1`)
  * paste0(`r out$v2`,"=", `r out$e2`)

Argue whether the value of the penalty parameter equal to `r out$pen` allows to get the given estimates (just text, no code needed)
  
**Your answer**:
In a Lasso regression the constraint the parameters, excluding the intercept, must satisfy is
$$
\sum_{j=1}^p |\beta_j|\leq\lambda
$$
where $\beta_j$ is the parameter associated to covariate $X_j$ and $\lambda$ is the penalty tuning parameter in the Lasso regression. Since `r abs(out$e1)+abs(out$e2)` is ?? than `r out$pen` we have that ???

## (b) 
Consider the variables `r out$v1` and `r out$v2`. Discuss whether or not the variables should be standardized prior to fit a Lasso regression model. You can use both code and text. Motivate your answer

```{r echo=TRUE}
library(SLDS2425)
data(fiberbits)
slds.fun(matricola, 3, fiberbits)
```

**Your answer**:
```{r echo=TRUE}

```

## (c)
Split the data set into a  traning and test data sets. Estimate a statistical model for **the chance of a customer leaving** by using shrinkage methods. Select the best model according to a suitable criteria. 

**Your answer**:
```{r echo=TRUE}

```

## (d)
Build a classifier to identify the customers who are most likely to quit. Compare the performance of the classifier, using suitable metrics, with the classifier considered in point (b). Comment on the results.

**Your answer**:
```{r echo=TRUE}

```


# Session Info

Do not modify the chunk below
```{r}
Sys.info()
#last compiled
date()
#student info
paste(name, surname, "-", matricola, sep=" ")
```
